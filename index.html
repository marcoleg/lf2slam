<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LF²SLAM: Learning-based Features For Visual SLAM</title>
  <link rel="stylesheet" href="style.css">

  <!-- Icona base -->
  <link rel="icon" href="favicon.ico" type="image/x-icon">

  <!-- Icone per diversi dispositivi e dimensioni -->
  <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">

  <!-- Icona per dispositivi Apple -->
  <link rel="apple-touch-icon" href="apple-touch-icon.png">

  <!-- Manifest per applicazioni web -->
  <link rel="manifest" href="site.webmanifest">
</head>
<body>
  <header>
    <h1>LF²SLAM: Learning-based Features For Visual SLAM</h1>
    <p class="authors">
      Marco Legittimo<sup>1</sup>, Francesco Crocetti<sup>1</sup>, Mario Luca Fravolini<sup>1</sup>, Giuseppe Mollica<sup>2</sup>, Gabriele Costante<sup>1</sup>
    </p>
    <p class="affiliations">
      <sup>1</sup>Department of Engineering, University of Perugia, Perugia, Italy & <sup>2</sup>ART Spa
    </p>
    <div class="buttons">
      <a href="https://ieeexplore.ieee.org/abstract/document/10801935" target="_blank" class="button">
        <img src="./pdf.svg" alt="PDF Icon" class="icon"> Paper
      </a>
      <a href="https://www.youtube.com/watch?v=kLlG4tYdzh8" target="_blank" class="button">
        <img src="./youtube.svg" alt="YouTube Icon" class="icon"> Video
      </a>
      <a href="https://github.com/isarlab-department-engineering/LFFS" target="_blank" class="button">
        <img src="./github.svg" alt="GitHub Icon" class="icon"> Code
      </a>
    </div>
  </header>

  <main>
    <section class="video">
      <div class="video-container">
        <iframe 
          width="800" 
          height="450" 
          src="https://www.youtube.com/embed/kLlG4tYdzh8" 
          title="YouTube video player" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen>
        </iframe>
      </div>
    </section>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        Autonomous robot navigation relies on a robot's ability to understand its environment for localization, typically using a Visual Simultaneous Localization And Mapping (VSLAM) algorithm that processes image sequences. While state-of-the-art methods have shown remarkable performance, they face limitations: geometric VSLAM algorithms depend on carefully tuned hand-crafted feature extractors, whereas data-driven approaches suffer from limited generalization and require extensive training datasets.

        LF²SLAM proposes a hybrid solution that combines the strengths of both paradigms by integrating a deep neural network for feature extraction with a traditional VSLAM pipeline. The system leverages a novel loss function to train sparse keypoint detectors, optimized for pose estimation tasks, within the Monodepth2 framework. Experimental evaluations demonstrate the system's robustness in challenging scenarios, such as low-light and low-texture environments, significantly outperforming ORB-SLAM3 in accuracy and generalization capabilities.
      </p>
    </section>

    <section class="overview">
      <h2>Overview</h2>
      <div class="overview-content">
        <img src="./overview.png" alt="Overview of LF²SLAM compared to ORB-SLAM3" class="overview-image">
        <p class="overview-text">
          LF²SLAM addresses the challenges of traditional Visual SLAM algorithms by integrating data-driven feature extraction with geometric pipelines. The system leverages the Superpoint network for sparse keypoint detection and the Monodepth2 training framework, enabling robust performance in environments with poor lighting and low texture. This hybrid approach minimizes reliance on extensive parameter tuning, offering a balanced solution that combines the adaptability of deep learning with the reliability of geometric methods, making it well-suited for real-world robotics applications.
        </p>
      </div>
    </section>

    <section class="method">
      <h2>Proposed Method</h2>
      
      <!-- Sottosezione 1 -->
      <div class="method-content">
        <h3>SLAM-specific feature extractor</h3>
        <img src="./final_pipeline_training.jpg" alt="Training LF²SLAM Pipeline" class="method-image">
        <p class="method-text">
          LF²SLAM employs a novel training procedure that integrates the Superpoint feature extractor into the Monodepth2 framework, enabling the detection of sparse keypoints optimized for visual odometry. This is achieved by modifying the standard photometric loss function to focus specifically on the most salient keypoints, using a binary heatmap generated by the Superpoint network. The heatmap identifies pixels with high probability of being keypoints, and these are used to compute a reprojection loss that minimizes pose estimation errors.

          The training process incorporates regularization terms to ensure robustness and prevent trivial solutions, such as suppressing keypoint detection entirely. One term enforces the selection of a specific number of keypoints, while another ensures consistency with a pre-trained Superpoint model. 
        </p>
      </div>
      
      <!-- Sottosezione 2 -->
      <div class="method-content">
        <h3>Integration within ORBSLAM3</h3>
        <img src="./pipeline_evaluation.jpg" alt="Integration of LF²SLAM within ORBSLAM3" class="method-image">
        <p class="method-text">
          Once optimized, the trained network is embedded into ORB-SLAM3, replacing the traditional ORB feature extractor. This allows LF²SLAM to leverage its learned features for accurate pose estimation, improving performance in scenarios where hand-crafted features often fail, such as low-light or low-texture environments.
        </p>
      </div>
    </section>

    <section class="results">
      <h2>Qualitative Results</h2>
      <div class="results-content">
        <img src="./traj.png" alt="Trajectories comparison between LF²SLAM and ORBSLAM3" class="results-image">
        <p class="results-text">
          LF²SLAM was evaluated on datasets like KITTI, EuRoC, and UNILAB, demonstrating significant improvements over ORB-SLAM3. The system maintained trajectory accuracy in challenging scenarios, including low-light conditions. In KITTI/09 (a), ORBSLAM3 (green) partially drifts, while LF²SLAM (red) achieves better performance. Both algorithms perform well on EuRoC/MH_05 (b). In UNILAB/C1 (c), ORBSLAM3 fails to track part of the ground truth, while LF²SLAM shows superior accuracy. Similarly, on UNILAB/X2 (d), LF²SLAM achieves a much better trajectory estimation where ORBSLAM3 almost completely loses tracking.
        </p>
      </div>
    </section>
  </main>

  <footer>
    <p>Created by Marco Legittimo <a href="https://isar.unipg.it/marco-legittimo/" target="_blank">Profile</a></p>
  </footer>
</body>
</html>
