<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LF²SLAM: Learning-based Features For Visual SLAM</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>LF²SLAM: Learning-based Features For Visual SLAM</h1>
    <p class="authors">
      Marco Legittimo<sup>1</sup>, Francesco Crocetti<sup>1</sup>, Mario Luca Fravolini<sup>1</sup>, Giuseppe Mollica<sup>2</sup>, Gabriele Costante<sup>1</sup>
    </p>
    <p class="affiliations">
      <sup>1</sup>Department of Engineering, University of Perugia, Perugia, Italy<br>
      <sup>2</sup>ART Spa
    </p>
    <div class="buttons">
      <a href="https://ieeexplore.ieee.org/abstract/document/10801935" target="_blank" class="button">
        <img src="./pdf.svg" alt="PDF Icon" class="icon"> Paper
      </a>
      <a href="https://www.youtube.com/watch?v=kLlG4tYdzh8" target="_blank" class="button">
        <img src="./youtube.svg" alt="YouTube Icon" class="icon"> Video
      </a>
      <a href="https://github.com/isarlab-department-engineering/LFFS" target="_blank" class="button">
        <img src="./github.svg" alt="GitHub Icon" class="icon"> Code
      </a>
    </div>
  </header>

  <main>
    <section class="video">
      <div class="video-container">
        <iframe 
          width="800" 
          height="450" 
          src="https://www.youtube.com/embed/kLlG4tYdzh8" 
          title="YouTube video player" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen>
        </iframe>
      </div>
    </section>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
        Autonomous robot navigation relies on Visual Simultaneous Localization And Mapping (VSLAM) for pose estimation, leveraging geometric or data-driven methods. LF²SLAM combines deep neural networks with standard VO pipelines to extract robust features and improve generalization in challenging scenarios. This hybrid approach demonstrates enhanced performance in environments with poor lighting and low texture, outperforming traditional ORB-SLAM3 in terms of robustness and accuracy.
      </p>
    </section>

    <section class="overview">
      <h2>Overview</h2>
      <img src="./overview.png" alt="Overview Image" class="overview-image">
      <p>
        LF²SLAM integrates learned features into VSLAM pipelines, addressing the limitations of traditional hand-crafted feature extractors. This architecture combines Superpoint and Monodepth2 to deliver superior robustness in challenging environments.
      </p>
    </section>

    <section class="method">
      <h2>Proposed Method</h2>
      <img src="./final_pipeline_training.jpg" alt="Training Pipeline Image" class="method-image">
      <p>
        LF²SLAM employs a novel loss function tailored to pose estimation tasks. By integrating the Superpoint feature extractor with the Monodepth2 framework, it trains a sparse keypoint detector optimized for visual odometry. The learned model is then embedded into ORB-SLAM3, replacing traditional ORB features, enabling robust and accurate pose estimations.
      </p>
    </section>

    <section class="results">
      <h2>Results</h2>
      <img src="./results_placeholder.png" alt="Qualitative Results Image" class="results-image">
      <p>
        LF²SLAM was evaluated on datasets like KITTI, EuRoC, and UNILAB, demonstrating significant improvements over ORB-SLAM3. The system maintained trajectory accuracy in challenging scenarios, including low-light conditions.
      </p>
    </section>
  </main>

  <footer>
    <p>Created by Marco Legittimo <a href="https://isar.unipg.it/marco-legittimo/" target="_blank">Profile</a></p>
  </footer>
</body>
</html>
